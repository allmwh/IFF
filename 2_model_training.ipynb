{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cb2425",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import trange\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96dd30fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# git for functions loading and work path finding\n",
    "import git\n",
    "\n",
    "repo = git.Repo('.', search_parent_directories=True)\n",
    "work_path = Path(repo.working_tree_dir)\n",
    "if str(work_path) not in sys.path:\n",
    "    sys.path.append(str(work_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e6d525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# package for model training\n",
    "# logging\n",
    "from comet_ml import Experiment\n",
    "\n",
    "# tokenizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "# transformer encoder\n",
    "# https://github.com/fkodom/transformer-from-scratch\n",
    "# https://medium.com/the-dl/transformers-from-scratch-in-pytorch-8777e346ca51\n",
    "from function.dlcode.atten import TransformerEncoder as TransformerEncoderScratch\n",
    "\n",
    "# cosine lr rate\n",
    "# https://github.com/katsura-jp/pytorch-cosine-annealing-with-warmup\n",
    "from cosine_annealing_warmup import CosineAnnealingWarmupRestarts\n",
    "\n",
    "# mocov3\n",
    "# code from Facebook's Github licensed by CC BY-NC 4.0 with slightly modified\n",
    "# https://github.com/facebookresearch/moco-v3\n",
    "import function.dlcode.moco_builder as moco_builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6fc239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some comet.ml logging code\n",
    "if __name__ == '__main__' and '__file__' not in globals():\n",
    "    logging = False\n",
    "    if logging:\n",
    "        api_key = \"\"  # comet api key used for logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d11b308",
   "metadata": {},
   "source": [
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f009ecee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqProcess():\n",
    "\n",
    "    def __init__(self, repeat_padding_target_length=512):\n",
    "        le = LabelEncoder()\n",
    "        le.fit(list('OACDEFGHIKLMNPQRSTVWY'))\n",
    "        self.encoder = le\n",
    "        self.repeat_padding_target_length = repeat_padding_target_length\n",
    "\n",
    "    def seq_process_pipe(self, seq_list):\n",
    "        encoded_seq_list = []\n",
    "        for seq in seq_list:\n",
    "            #repeat_string\n",
    "            seq = self.repeat_string(\n",
    "                seq, target_length=self.repeat_padding_target_length)\n",
    "            #add cls token 'O'\n",
    "            seq = 'O' + seq\n",
    "            #encode\n",
    "            seq = self.encoder.transform(list(seq))\n",
    "            encoded_seq_list.append(seq)\n",
    "        encoded_seq = torch.tensor(np.stack(encoded_seq_list))\n",
    "\n",
    "        return encoded_seq\n",
    "\n",
    "    def repeat_string(self, a_string, target_length=256):\n",
    "        '''\n",
    "        https://www.kite.com/python/answers/how-to-repeat-a-string-in-python\n",
    "        '''\n",
    "        number_of_repeats = target_length // len(a_string) + 1\n",
    "        a_string_repeated = a_string * number_of_repeats\n",
    "        a_string_repeated_to_target = a_string_repeated[:target_length]\n",
    "        return a_string_repeated_to_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d9b296",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqDataset(Dataset):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.df = df\n",
    "        self.homology_ids = df['homology_id'].unique()\n",
    "        self.seqprocess = seqprocess\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.homology_ids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        homology_id = self.homology_ids[index]\n",
    "\n",
    "        # get human q\n",
    "        q = self.df[(self.df['homology_id'] == homology_id) & (self.df[\"is_human_seq\"] == 1)]\n",
    "        q_seq = q['frag_seq'].tolist()\n",
    "        q_seq_tokened = self.seqprocess.seq_process_pipe(q_seq)\n",
    "\n",
    "        # get k as postive sample\n",
    "        q_as_k = self.df[self.df['homology_id'] == homology_id]\n",
    "        q_as_k_seq = q_as_k.sample(1, replace=True, weights='q_prob_pure')['frag_seq'].tolist()\n",
    "        q_as_k_seq_tokened = self.seqprocess.seq_process_pipe(q_as_k_seq)  # max_length + encode\n",
    "\n",
    "        return {\n",
    "            \"q\": {\n",
    "                'token': q_seq_tokened,\n",
    "                'seq': q_seq,\n",
    "            },\n",
    "            \"k\": {\n",
    "                'token': q_as_k_seq_tokened,\n",
    "                'seq': q_as_k_seq,\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def collate_fn(self, data):\n",
    "        q_seq_tokened, q_seq, k_seq_tokened, k_seq, = \\\n",
    "        zip(*[(s['q']['token'], s['q']['seq'],\n",
    "               s['k']['token'], s['k']['seq'],\n",
    "               ) for s in data])\n",
    "        q_seq_tokened = torch.stack(q_seq_tokened).squeeze(dim=1)\n",
    "        k_seq_tokened = torch.stack(k_seq_tokened).squeeze(dim=1)\n",
    "\n",
    "        return {\n",
    "            \"q\": {\n",
    "                'token': q_seq_tokened,\n",
    "                'seq': q_seq,\n",
    "            },\n",
    "            \"k\": {\n",
    "                'token': k_seq_tokened,\n",
    "                'seq': k_seq,\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817795e7",
   "metadata": {},
   "source": [
    "# network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fddadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttenTorchScratch(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(AttenTorchScratch, self).__init__()\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.atten_mlp_ratio = atten_mlp_ratio\n",
    "        self.depth = depth\n",
    "        self.num_heads = num_heads\n",
    "        self.seq_length_with_cls = seq_length + 1\n",
    "        self.num_tokens = 21\n",
    "\n",
    "        self.embed = nn.Embedding(self.num_tokens, self.embed_dim)\n",
    "        self.atten = TransformerEncoderScratch(\n",
    "            num_layers=self.depth,\n",
    "            dim_model=self.embed_dim,\n",
    "            num_heads=self.num_heads,\n",
    "            dim_feedforward=self.embed_dim * self.atten_mlp_ratio,\n",
    "            dropout=0.1,\n",
    "        )\n",
    "        self.head = nn.Identity()\n",
    "\n",
    "    def forward(self, x, return_all_atten=False):\n",
    "\n",
    "        x = self.embed(x)\n",
    "        x, all_atten = self.atten(x, return_all_atten)\n",
    "        cls = x[:, 0]\n",
    "        x = self.head(cls)\n",
    "\n",
    "        return x, all_atten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a0e6f1",
   "metadata": {},
   "source": [
    "# training param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316b5d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# log, we used git commit id to log our experinment records\n",
    "checkpoint_id = repo.commit().hexsha[:7]\n",
    "memo = \"temp_0.2_length_512_newdataset\"\n",
    "\n",
    "# loading dataset\n",
    "# please change the path downloaded from OSF: https://osf.io/jk29b/\n",
    "if __name__ == '__main__' and '__file__' not in globals():\n",
    "    dataset_path = work_path / \"1_prepare_training_data\" / '1-3_vsl2_omaseq_with_prob.pkl'\n",
    "    relative_dataset_path = str(dataset_path.relative_to(work_path))\n",
    "    df = pd.read_pickle(dataset_path)\n",
    "\n",
    "    # q_prob_pure is NaN indicating that there is only one frag sequnce in the homolog,\n",
    "    # and can not perform contrastive pretext task\n",
    "    df = df[df['q_prob_pure'].notnull()].reset_index(drop=True)\n",
    "\n",
    "# dataset + dataloader\n",
    "batch_size = 50\n",
    "seq_length = 512  # seq_length + CLS = 513\n",
    "\n",
    "# BaseEncoder transformer\n",
    "depth = 6\n",
    "num_heads = 8\n",
    "embed_dim = 128\n",
    "atten_mlp_ratio = 4\n",
    "moco_mlp_dim = 128\n",
    "\n",
    "# MoCo based\n",
    "m = 0.999  # momentum\n",
    "nce_temp = 0.2\n",
    "\n",
    "# training param\n",
    "max_epochs = 400\n",
    "lr = 1e-3\n",
    "warm_up_step = 10\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2afa7e5",
   "metadata": {},
   "source": [
    "# logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7452489d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for log\n",
    "if __name__ == '__main__' and '__file__' not in globals():\n",
    "    if logging:\n",
    "        hyper_params = {\n",
    "            \"atten_mlp_ratio\": atten_mlp_ratio,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"seq_length\": seq_length,\n",
    "            \"embed_dim\": embed_dim,\n",
    "            \"moco_mlp_dim\": moco_mlp_dim,\n",
    "            \"depth\": depth,\n",
    "            \"num_heads\": num_heads,\n",
    "            \"moco_momentum\": m,\n",
    "            \"max_epochs\": max_epochs,\n",
    "            \"learning_rate\": lr,\n",
    "            \"warm_up_step\": warm_up_step,\n",
    "            \"nce_temp\": nce_temp,\n",
    "            \"use_dataset\": relative_dataset_path,\n",
    "            \"checkpoint_id\": checkpoint_id,\n",
    "        }\n",
    "\n",
    "        exp_name = '{}-{}-mocov3'.format(checkpoint_id, memo)\n",
    "        experiment = Experiment(api_key=api_key,\n",
    "                                project_name=\"mocov3\",\n",
    "                                log_code=True,\n",
    "                                auto_metric_logging=False,\n",
    "                                auto_param_logging=False)\n",
    "\n",
    "        experiment.set_name(exp_name)\n",
    "        experiment.log_parameters(hyper_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01e0af3",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60388b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "seqprocess = SeqProcess(repeat_padding_target_length=seq_length)\n",
    "if __name__ == '__main__' and '__file__' not in globals():\n",
    "    seqdataset = SeqDataset()\n",
    "    train_dataset, _ = random_split(seqdataset, [28892 - 42, 42])\n",
    "    train_dataloader = DataLoader(train_dataset,\n",
    "                                  batch_size=batch_size,\n",
    "                                  shuffle=True,\n",
    "                                  collate_fn=seqdataset.collate_fn,\n",
    "                                  num_workers=os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc7daae",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#training, only trining\n",
    "if __name__ == '__main__' and '__file__' not in globals():\n",
    "\n",
    "    model_save_path = str(work_path / \"trained_weight.pt\")\n",
    "    moco = moco_builder.MoCo(base_encoder=AttenTorchScratch,\n",
    "                             dim=embed_dim,\n",
    "                             mlp_dim=moco_mlp_dim,\n",
    "                             T=nce_temp).to(device)\n",
    "\n",
    "    optimizer = optim.AdamW(moco.parameters(), lr=lr)\n",
    "    scheduler = CosineAnnealingWarmupRestarts(optimizer,\n",
    "                                              first_cycle_steps=30,\n",
    "                                              cycle_mult=1.0,\n",
    "                                              max_lr=lr,\n",
    "                                              min_lr=lr / 100,\n",
    "                                              warmup_steps=warm_up_step,\n",
    "                                              gamma=1.0)\n",
    "\n",
    "    #training\n",
    "    for current_epoch in trange(max_epochs):\n",
    "        training_loss = []\n",
    "        moco.train()\n",
    "        for batch in tqdm(train_dataloader, desc=\"train\", leave=False):\n",
    "\n",
    "            q = batch['q']['token'].to(device)\n",
    "            k = batch['k']['token'].to(device)\n",
    "\n",
    "            loss = moco(q, k, m)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            training_loss.append(loss.cpu().detach())\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        training_loss = torch.stack(training_loss).mean().numpy()\n",
    "\n",
    "        if logging:\n",
    "            experiment.log_metric(name=\"training_loss\",\n",
    "                                  value=training_loss,\n",
    "                                  epoch=current_epoch)\n",
    "            experiment.log_metric(name=\"learning_rate\",\n",
    "                                  value=round(scheduler.get_lr()[0], 12),\n",
    "                                  epoch=current_epoch)\n",
    "\n",
    "        torch.save(moco.state_dict(), model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b77b23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#close comet ml\n",
    "if __name__ == '__main__' and '__file__' not in globals():\n",
    "    if logging:\n",
    "        experiment.end()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
